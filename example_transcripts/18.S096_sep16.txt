00:12
At the very end, after we expose some of the basic properties of what this definition is and how it gives us a new viewpoint to think about some of the things we've already talked about.
00:22
Like what it means for matrix to be in vertical. We're going to talk about a concrete application.
00:28
So I think, you know, maybe we should really retitle this course, because it's not just linear algebra and optimization somehow
00:35
Our goal is to teach you how to use linear algebra and optimization of think about the world around you. So now that we've built up this repertoire basic linear algebra concepts.
00:44
It'll be amazing how many different things we can fit under this umbrella through our new understanding
00:50
So today we're going to explore a key definition and this key definition, it will be abstract at first, but we'll go through some examples.
00:57
It's going to help us think about how you know how to view matrices as operators in new ways. And some of the properties that we've already discussed. So let me just tell you what it is. It's the notion of a vector space.
01:10
So vector space.
01:14
Well, it's a set and will be interested in it specifically over the reels, although you can have it over other things like the complex numbers and other fields.
01:24
While a vector space is a set, which will denote by V and the important thing about it is that it has two key notions which give it structure. So the two key notions associated with a vector space. Are you need to have a definition for how to add
01:45
So this means that, you know, for any VM w that you give me in this vector space, we must have some way to define the plus w and that must also be in the original vector space.
02:00
And the other thing we need to do that gives it structure is we need some way to scale up and down vectors.
02:08
So what this means, here is that, you know, for any vector in the vector space for any real coefficient alpha. So, any real scale or alpha
02:18
You need to have a way of defining what it means to multiply the vector v by the scale or alpha and that again needs to go back into the original vector space.
02:30
So this is the key thing about a vector space as it's just a set with two key properties that is going to be able to give it a lot of powerful structure.
02:42
So I claim that this isn't anything new. It's just the more abstract way of saying things that we've already done and some concrete examples.
02:49
Because we've already seen some examples of vector space as an action. So if you remember, on the very first day we talked about the set RD. This was just the set of length, the vector is the dimensional vectors which we define there's just a set of all d two bolts.
03:11
Of reels.
03:13
We got them just by putting those square brackets and stacking D real numbers on each other.
03:19
But in the very first lecture, we already had some structure associated to the set of D tools because you know what we actually did with them was we added these vectors to each other, we define the right notion
03:30
We multiply them by a scale. And we did some sort of hybrid operations of, you know, doing a bit of both, which we called linear combinations. So vector spaces are already something that we've seen. And they're pretty familiar. This is just an abstract way of saying it.
03:46
And actually, the key thing today is we're going to do something very powerful with this notion of vector spaces.
03:52
Which is that we're going to derive geometric insights about matrices revisiting things we've already talked about like what it means for a matrix to be infertile.
04:02
From the perspective of properties of their vector spaces actually will care about is they'll be something called sub spaces that are really subsets of vector spaces.
04:12
And we're going to take any matrix and associate some fundamental sub spaces to them and talk about the relationship of the sub spaces, the things that we've already seen.
04:21
So let me tell you the other key definition for today, which is the notion of a subspace. So you start off with some vector space. So the is a vector space.
04:33
So, it satisfies the definitions above and what will be interested in is the notion of a subspace ass, which is a subset of the
04:43
But the key thing is that it needs to have its own nice properties that give it order and structure. So in order for it to be a subspace.
04:53
It better inherit those same properties of the original vector space that you can define edition and scaling.
05:01
So in particular, you again within the subset ass just restricted to the subset S. You need to be able to define
05:09
What edition means that if you take any BMW, which are in the subset S You better have the property that v plus W. Well, you know, that's in the original vector space fee.
05:19
Because that's the definition of a vector space, but you need something stronger that it's actually in the subset S, the subspace.
05:29
And the same way. You need to be able to define what multiplication and so you take any V that's in the subspace s
05:36
And any scale or alpha that's a real number because these are vector spaces over the reels and you better have the property that alpha times B is on the subsets.
05:47
So these are just the right closure properties you take a vector space Vive like the set of all the tools of real numbers.
05:53
And we're interested in sub spaces that have the same vector space structure. So just as a thought experiment that gets some familiarity with working with this definition of sub spaces.
06:04
Let's talk about, you know, our two and what are the sub spaces of the standard two dimensional vectors.
06:11
So in fact, you know, it's always hard and when I give you some abstract definition is getting the corner cases. Right.
06:18
You know, if I ask a question like what are the sub spaces of our two if you know the set of two dimensional vectors.
06:24
Lot of times the things that you'll miss will be the trivial things. But let me do that job for you and give you the trivial ones. So one vector space as something kind of boring as just the singleton vector zero
06:37
That's a really boring vector space, but it is indeed a vector space, and it is indeed a subspace of our two
06:44
Because you can definitely define what it means to add 02 itself. You can definitely define what it means to multiply by scholar, because all you ever get zero. That's it.
06:53
It's kind of boring. Well, you can do the other boring thing which is you could also look at the subspace, which is our to the entire Euclidean plane.
07:03
So S equals V and the definition above. So the subspace doesn't really have any new properties. It's just the same thing, it was before. It's all two dimensional vectors.
07:11
With the familiar notion of adding vectors and multiplying by scaling. Now, the more interesting thing is what happens in between the empty set, and the full Euclidean plane.
07:23
So what are the non trivial examples of subspace is here. So, there is something else interesting that happens, which is you can take ass to be the set of points on any line that's passing through the origin.
07:46
And, you know, just the check your notion you know that your understanding of the definitions are right.
07:51
Well, you have to have that the line is passing through the origin. Because if you think about this definition of a subspace.
07:58
What happens if I multiply by zero, I better get the zero vector. Again, that's why you better include the origin. So these are all examples of the sub spaces of our to that you can get
08:09
And actually in the next lecture when public talks about dimension will think about, you know, ways to classify these different sub spaces.
08:18
But in any case, the key property of sub spaces that makes them really useful from a linear algebraic standpoint as a way to think about what matrices are doing to vectors.
08:28
Is because they're not merely a definition there a way to build up more complicated things. So the key property is that vector spaces and sub spaces in general they actually lend themselves to convenient and useful ways to combine them to get new ones.
08:46
So let's talk about you know what we can and cannot do. When we have to sub spaces and we want to combine them to get a new substance.
08:55
So my first question for you, as you know, what if I want to combine them by taking their union.
09:01
So what if I have to sub spaces s one s to let's still work with our underlying vector space of the Euclidean plane.
09:09
So then to remember from above our sub spaces S one and so those are aligned passing through the origin.
09:15
It's all of the points along this line s one, you can take any two of those vectors and add them and you still get something on that same line.
09:23
The same thing is true for us to you can scale things up or down and you're just dilating where they are on the line. So these are indeed sub spaces. But now the question is, what if I take these two sets. There are two subsets of the plane is wanting us to and I take their union.
09:40
So the set of all points that I'm going to get is I get everything that's on these two highlighted directions. I don't get anything in between.
09:51
So, you know, does taking the union of two subspace, give us another subspace. So the answer to this is no. Right.
10:03
And you should think about you know what property goes wrong, because if I do something like I take a vector V that's on the subspace S one and I take a vector w that's on the subspace as to
10:18
What happens if I do something like the plus w i might actually get a point way out here and this could be the plus W. It's something that lies in between the lines.
10:29
So in this way, even though addition as well defined for vectors and my overall vector space V. It doesn't actually take me back within the set S wer s is equal to s one union so
10:41
So unions are not allowed. Those are not allowable ways to create new sub spaces, but there is something that you can do. So what happens if you take s one intersect. So
10:53
Then the answer here as I was foreshadowing is that yes, that really is a subspace. Now, sometimes you're going to get boring things
11:01
Because, like, in this case, when I take the intersection of S one, S two. I definitely get a subspace. I get the trivial subspace that I started off with, which is just the single vector zero
11:12
And that is indeed a substance, but you can do much more interesting things, which is, you know, what happens if I have my underlying vector spaces are three. So it's the set of all three two pools of factors.
11:24
Then I could have sub spaces that look like things like planes and 3D and when I take the intersection of planes. Now I get lines which are still sub spaces.
11:34
So you can build all kinds of interesting geometric objects by taking the intersections and doing other kinds of natural operations on subspace.
11:44
And matrices are going to give us a natural way in which we can and should build up different kinds of sub spaces and reason about their properties.
11:53
So that's where we're headed is we're going to just explore this definition and what it means, especially for some key concepts and linear algebra that we've already introduced over the earlier lectures.
12:05
But the first thing that I want to do is, you know, sometimes it's useful to actually try and prove these facts or give a convincing argument about why they're true
12:14
Because that's a good way to check that you're using the properties in the definitions and the right way.
12:19
And that you're not just following some geometric intuition, which might or might not be correct. So let's do that just to make sure that we're comfortable with this definition. So let's codify this fact. So what happens if we take s one and two.
12:36
And let's assume that these are sub spaces.
12:43
Then what I claim is that
12:47
So is there intersection S one intersect. So
12:53
And so the way that we're going to do this as we only really have one thing that we can do. We have to verify the definitions of what it means to be a subspace.
13:01
So this is a pretty boring proof, a pretty boring argument. We're just checking that each one of the criteria in the definition works and it's just this little checklist that we go down.
13:11
But we want to be clear about each property with revoking is actually something that we're allowed and we do know does generally hold
13:18
So let's start off with the first property we want to check. So imagine that I give you two vectors, a BMW that really are in the set S which is defined to be the intersection between S one, S to
13:32
Now we're trying to decide whether mass is a vector space. So what we want to know is whether the plus w is also in the set S. So let's check it.
13:42
That was pretty easy. Well, by definition, we know that VM W are in the set S one, because they're in the intersection between S one, S two and we also and that together with the property that s one is a Wagner's subspace means that the plus w is also in f1
14:03
Same thing holds for us to write the NW are definitely an S to because they're in the intersection of s one s do that with the fact that st is a subspace, by assumption means that the plus w is also in the same thing.
14:18
So these two things together, what did I mean they tell us exactly what we're after the plus w really is in S one intersect. So
14:28
That's good. That's the first property of a subspace. We're almost home. What's the other property. It's actually the same things. I just wrote it out already. Because that's kind of boring.
14:37
But you take the and, you know, that is an S one and it's an S to. So multiplying by alpha puts it in S one puts it in S, too. So it's an intersection. So that means that multiplying by alpha
14:50
Really is. Okay. It's an allowable operation that puts us back within the subset that we've now proven really is a subspace.
15:00
So that's cool. This gives us a way to build different sub spaces out of ones that we started off with
15:05
And now we can get to the meat of today's lecture is what we can do with these definitions.
15:11
So how do we think about matrices and fundamental sub spaces associated with them. So, in this lecture, we're just going to talk about two of the fundamental sub spaces, there really for and we'll talk about those later but these two are already going to give us a lot of insights
15:28
So let's talk about the first fundamental subspace, which is the notion of the column space. So now we come back to matrices. So given a matrix. Let's call it a
15:41
And let's say that it has columns.
15:47
A one, A two, all the way to am so this matrix A has an columns them.
15:58
The column space.
16:05
Of a which, you know, we're going to use it so much that we're actually going to give it a special you know notation gets see have a what it is.
16:16
Is it's the find to be the set of all things that you get as linear combinations of the columns of it.
16:23
So you can take alpha one times a one and you can add to that alpha two times a to all the way up to alpha m times am
16:33
And you do this for all possible sailors Alpha Phi that are different real values.
16:40
So this we've already introduced a few times. This is just the set of all things that I can get from linear combinations of the columns of thing.
16:49
That's what the columns basis. That's our first key definition, the column space is just everything that you can get by taking a and multiplying it by a vector of alpha coefficients.
17:04
So, to put it another way. It's the set of all linear combinations of the column of A. Actually, some other terminology that sometimes you'll hear is that it's called the span of the columns of A, something that we've already talked about.
17:18
And I claim that this is actually something now that we've introduced this definition. It's something that I've actually been hiding. And we've been talking about implicitly the entire course so far.
17:29
So, you know, let's go all the way back to the second lecture and think about how the things that we said can be formulated instead through the column space.
17:39
So on the second lecture. Way back when I gave you a linear system that we wanted to solve.
17:43
Back then, the only tool that we had at our disposal was the role view and the column view. So we tried to visualize these things geometrically and very concrete instances.
17:53
And I took this matrix A. That's two by two and I put some crazy right hand side for v1 and v2 where I put pie and zeta three and I asked you.
18:02
Is there a solution for this linear system. And I said that there is for any choice of b one and b to and that's what we argued about was we we reason about why for anyone v1 and v2. There is a unique solution to this linear system.
18:17
And later. We revisited this again with the language of Gaussian elimination, because what we did was we computed the inverse of the matrix, say, and that was a way to reason about why for any choice of V1 and V2, there is a solution.
18:32
But I claim. Another way to think about these things. Is that really what I'm saying. When I say that this linear system has a solution for any v1 and v2.
18:41
Is I'm just saying that the column space of A is all of our to every two dimensional vector, you can get as some linear combination of the columns.
18:52
That's a purely equivalent way to say what I said above about there being a solution for any v1 and v2.
19:00
So, more generally, let me just rephrase what I said in words, but in in fact going to stand alone fact if you're interested in any linear system, not just to buy tool in your system.
19:13
And you take the linear system A x equals b and here. What I'm talking about is x and to be here are vectors, just to be clear, they're not scale or coefficients. And what I claim is that this linear system has the property that it has a solution.
19:32
For a particular choice of be
19:38
If and only if be is in the column space of A. That's just a purely equivalent way to write it because they're both saying the same thing.
19:50
The column spaces, the set of things that you can get out when you take A times x. For some choice of x. And I'm just reformulating it using my definition of color space that this linear system has a solution. If an ollie FB is in the column space.
20:08
And now what I want to do is I want to understand the column space through the other key concept that we introduced earlier was the notion of the matrix inverse
20:18
So let's rethink you know what it means for a matrix to have an inverse, but using this notion of the column spaces that so for this. I'm going to need a key fact
20:28
Which is that, you know, I've already said before actually used it last time. So recall that if I take a being a square and in vertical matrix.
20:37
Then it's right and left in verses are the same. So if you give me a matrix be so there'd be times A is the identity matrix, the one with all zeros, but ones along the diagonal
20:48
That's the same thing as saying that that matrix be as the property that A times B is equal to the identity and actually face first. So this was one of the key properties that we used was that, you know, for a square matrix. It's right and left in verses are the same thing, if they exist.
21:07
And now let's go back to infertility and let's restate, some of the things that we know about in verses, but with the language of column spaces.
21:18
So here's the key facts now.
21:21
Is that what I claim is that if you start off with an A.
21:27
If A is square, things are always a bit more complicated for non square things and we'll get into that later if a square. And let's say that it's n by n.
21:42
Then what I claim is that the following two statements are equivalent.
21:47
A is in vertical. Remember, we can figure out whether something's in vertical by doing this Gauss Jordan elimination
21:54
To see if we can get something with just you know ones along the diagonal. And that was the way we computed the inverse
22:00
But I claim is that that statement about what goes Jordan elimination finds the fact that A's and vertical is actually equivalent to the fact that the column space of A is all of our him.
22:16
So, ultimately, to think about this geometrically, you know, what does it mean we have an n by n matrix. So they're n different n dimensional vectors. When we look at the columns of A.
22:26
And what it means for the column space of A to be all of our end means that we can get every single thing in aren't
22:32
So it means that what we get is really just the skewed grid that covers the entire n dimensional plane. And that's the same thing.
22:40
As ensuring that there's always a unique way to write down to solve a system A x equals b, there is a solution. And it's always unique and that's what the inverse needs.
22:51
So this is nice because it gives us a bunch of ways to revisit some of the things that we already talked about. But using these new definitions.
23:02
So let's think about you know why this fact is true because you might have some geometric intuitions about why it's true, but it's good again to see some kind of argument about what's going on here.
23:12
So let's think about how to prove this fact, right. So let's think about why it's true. Let's think about One Direction. First, which is, you know, what happens if you know I have a square matrix A that's n by n. And I tell you that a isn't vertical
23:32
What I want to do now is I want to show that actually the column space of A is all of RM. So what I really want to show is that for any choice of be which is an n dimensional vector that it actually can be expressed as a linear combination of the columns and
23:52
That's one direction is I want to take the fact that as a vertical and conclude the fact that the column space of as all of our it because for any beat that you give me I should be able to write it down as a linear combination of the columns of it.
24:07
So I guess the main question really is what linear combination. Should I use this is actually pretty easy.
24:14
Because let me guess, the right answer. It's kind of nice when you can guess the right answer. So let me just try out X is equal to A inverse times b.
24:25
You give me a, b, and you asked me to write it down as a linear combination of the columns of A, and I'm going to use this x I just multiply it by A inverse. And I get my x
24:35
And then what I claim is that when I choose this x. Let's write it out. You know what happens when I take that linear combination of the column survey.
24:44
I plug in the value for x that I've sent and it's equal to eight times A inverse times b.
24:52
But now I can use associate activity. The key property of matrix multiplication and eight times A inverse. Because, you know, a is a square matrix that's right and left in verses are the same.
25:04
That means that there's no ambiguity in this statement. This is the identity matrix times b, which is the same thing as be
25:13
So that's the punch line right is when you give me an inverse, I can tell you for any be that you give me what is the linear combination of the column is a day. That gives me
25:25
And hence I can get anything as a linear combination. That's one direction. All right.
25:33
Actually, the second direction is actually takes a little bit more of, you know, a good idea. So
25:41
The converse is a little bit trickier but let me explain how this works. First, let me just remind you about what the setting is
25:48
So the conference here means that you know we take a square matrix that's n by n. And I'm giving you the assumption that the column space of A is all of our end
25:59
And what I want to do is I want to conclude from that fact that A has an inverse
26:04
That's kind of weird actually right in verses happened almost accidentally for us. They were something that just fell out of gas Jordan elimination before
26:13
And now they're going to fall out of the fact that column space of as all of our it. So that's actually kind of weird and
26:20
You know, that's why I'm emphasizing this part is not really obvious. So the question is, you know, if I know that the column space of A is all of our M.
26:28
How does that let me build an inverse of the matrix, say, and that's pretty weird actual. So how does this work well you know there's something really useful to do which has, I can consider
26:40
Something called the standard basis vectors you actually won't know why the term basis is in there until the next lecture.
26:47
But that's what they're called, and they're just very simple vectors, because they go along the x axis, the y axis and the Z axis. So they're very simple
26:55
The first one just has a one in the first coordinate and the rest of it of zeros.
27:00
The second one has a one and the second coordinate the rest of it or zeros and so on. These are called the standard basis factors. They're all unit factors because they have length one.
27:09
And now what do I know, I know that he one is in the column space of it. That's my assumption because the column space of A is all of our end so it definitely contains a one.
27:22
So that means there has to be some vector x one that expresses he one as a linear combination of the columns of that. So I know that there must be some solution to this linear system x one equals 11
27:38
And I know that there must be some x two. So, that A times x to is equal to eat too.
27:44
And so
27:45
Now I claim, we've actually found an inverse in disguise.
27:51
And this, you know, just the perk up your ears. This is definitely related to one of your homework problems so
27:56
This is kind of, you know, one of the answers. If you look at it the right way.
28:00
So you should definitely pay attention to this part. It's kind of like when I teach discrete math or algorithms courses.
28:06
Anytime we do a problem and it shows up on a Google interview, all of a sudden when I say that everyone sits up and pays a lot of attention. And then after that, they get silent and play Pokemon Go is like discussed
28:19
But in any case, so now I know that there are X one, X two up to XM that solve these linear systems. And here's the awesome idea.
28:28
It's really simple, actually, what is my inverse. It's a times the matrix. If I look at eight times the matrix which I get from concatenate in all of these access together.
28:40
What do I know remember what matrix matrix multiplication looks like one interpretation of matrix matrix multiplication is that I take the first column of this right matrix.
28:51
And I take it times the matrix and that gives me the first column of the output which what is eight times x one THAT'S A one by assumption.
29:02
What's a times x two, that's eight to all the way up to em. And what happens when I concatenate the standard basis vectors together. He won, he to up to em, I get the identity. So this means that this matrix right here is in disguise, our influence
29:23
So that's really neat is just the fact that for a square matrix.
29:28
You can say something about the column space being all of our em, you can build the sudden, this is a different way to think about the inverse, that's a little bit different.
29:37
Than the way that we built the inverse out of gas Jordan elimination. Here we're building it out of solving the right linear systems. So this will take a little bit for you to really wrap your heads around because these are pretty strange concepts. The first time you see them.
29:53
But we found the inverse of a
29:56
And just as I'm you know comment I want to say that we actually now have a lot of linear algebra concepts under our belt. Right.
30:04
We have, you know, these geometric views of the road view that column view. If the notion of the inverse with matrix multiplication. We have the column of space.
30:13
What I would encourage you to do is what I'm doing for you in lecture is, you should go back through and think about the old linear algebraic things we talked about, but using your new tools.
30:23
Because the more that you can exercise thinking about it in many different ways, the better you'll be able to solve problems.
30:31
Because a lot of times their problems which you know you'll look at and the first approach that you try the first definition that you try
30:39
Won't quite work. But when you try a different definition that's equivalent from linear algebra. The thing will actually be very easy.
30:46
So that's something that's a very useful exercise that you should be doing, you know, on your own is really think about these things in as many ways as you can, not just one particular way.
30:58
Alright, so now let's get to the second fundamental subspace. And then we're going to have some, you know, fun with applications or rather anti applications. I'll just leave it mysterious like that for now. But the other second fundamental subspace is called the null space.
31:15
So the null space of a matrix A, which, again, we're going to introduce some special notation for it. It's MFA, all it is is the following. So an eBay.
31:29
Is the set of all exes. So that A times x is equal to zero.
31:38
So certainly, the vector zero is in this because I take a time zero, I get zero.
31:45
But in some cases you have other vectors x that you can multiply and get the old zero vector out. That's called the note space because you take a you hit it with something and it creates something that's no
31:56
And it's another fundamental subspace.
32:00
Now, just as some some intuition to wrap your head around this.
32:05
You know what happens if I take a slightly different definition. So if I take something like so the set of all access so that a x equals beats on a superset of all solutions through a given linear system.
32:17
Is this a subspace. Well, the way that I'm discussing this, I think you can intuit what the answer is. The answer is no.
32:24
But you know what's more important is why right so what property of the subspace doesn't violate the issue is that, you know, if you take a x one is equal to BS X one is in the set ass and you take a x two is equal to be. But when you add them.
32:43
You take A times x one plus x two. Well, what can I use the distributive property that's the same thing as x one plus x two, which is the same thing as B plus B and that's two times b.
32:56
So x one plus x two is not necessarily in the set ass unless the vector be is actually zero. That's the same thing we saw was before because you need the subspace to actually go through the origin for it to be a suspects.
33:11
And now what I claim is that now that we have these two fundamental notions of sub spaces. They're actually very interrelated. So the column space and the null space actually tell you a lot about each other.
33:24
So in fact, one of the questions that I'm asking is, you know, what happens
33:28
If I take a matrix A that square and there's a non zero vector x that's in the null space. So sometimes you can get more than just the single vector zero. So what if there's a non zero x that's in the middle space so that eight times x is equal to zero.
33:44
First, I claim that that tells us something about inverted ability
33:49
We can ask the question whether a isn't vertical and here are the answer is, you know, no.
33:55
Because let's think about what invert ability means. So one way to think about an inverted ability in a really colloquial but useful way is that what invert ability means is that you can undo.
34:08
Multiplying by the matrix say because there's another matrix A inverse that you can multiply by to get rid of the first multiplication.
34:17
But you know what happens if I have an X that's non zero and eight times x equals zero. The issue is that, you know, eight times x equals zero. And that's the same thing as a time zero.
34:28
So how can I possibly invert. I don't know if what I started with before I multiply by x was the by a was the vector x or was the vector zero
34:38
So this is the failure of having a unique way to undo matrix vector multiplication. So the fact that there's something non zero when you have a square matrix in the health space means that it is not in vertical
34:55
And actually, again, I really like, you know, thinking about this, not just abstractly, but through some concrete examples that we've seen.
35:04
And so let's do the same thing that we always do, which is, let's go back to our old examples and think about what these definitions in fact mean in this context.
35:13
So actually, again, and the second lecture, I guess we did a lot there. We talked about, you know, this matrix A, which is three by three and we talked about, you know, when there are instances where it doesn't have a solution x equals b.
35:29
And you know what we didn't some of the later lectures, we talked about Gauss Jordan elimination was the thing that we did to reason about solutions. The linear systems was we put it into reduce throw show on form.
35:40
You can go back in your notes and check what the reduced throw echelon form ones. And I claim that the thing that we did.
35:47
Was we figured out a way to write it as this matrix, right here, which now isn't reduced throw echelon form but critically. It has a row of zeros and what we're really doing was we found a matrix be so that we can express a equals b times a prime
36:05
The thing that we got from reduced throat echelon form.
36:09
And you know this matrix be somehow has to do with the operations that we do in Gauss Jordan elimination
36:15
Because what we're doing is we're multiplying the matrix A buy something on the left, the simplified until we got producer Echelon for
36:23
This matrix be is really the inverse of all of those operations. So, same kind of thing.
36:28
But the key point is that reduce throw Echelon forum is actually going to reveal a ton of interesting properties about this new definition that we have
36:36
About the null space. So I claim that from this expression we can actually figure out how to find a non zero vector in the null space.
36:47
So if I take this expression as a given in this concrete example where I can write a this three by three matrix. I started with as be times the southern matrix A prime. That's an reduced throw echelon form.
36:58
How can I know go about finding a non zero x so that A times x is equal to zero.
37:06
So here's the key point is that A times x being equal to zero is the same thing as a prime times x being equal to zero.
37:19
Because this matrix be ism vertical. It doesn't change which things you multiply to get zeros or non zeros.
37:28
So this means that my task is much simpler. Now, I had this matrix A and I wanted to find it smell space. But now I have a much simpler matrix A prime. That's an reduce throw Echelon for
37:39
That I want to find a small space. And let me show you how this works because it's going to look a lot like back substitution
37:46
Let me try out some vector. So it's going to have x y&z are the values and when I do this matrix vector multiplication. I'm going to get x plus six fifths times z.
37:59
That's the first entry. It's a scaling. The second thing I'm going to get as y plus four fifths times z. And the last thing I'm going to get a zero critically, because the last row in a prime is all zeros and I want this guy to be equal to all zeros.
38:19
So now I can figure out how exactly to do this because when I can try out as I can try out some value for x. Let's say I try x equals one.
38:28
And then I can use. For example, the first equation x plus six fifth z is equal to zero to solve for what Z had better be so z is equal to minus five, six.
38:39
And then I can plug that in and figure out what why is supposed to be equal to if I've done it right, it's supposed to be equal to two thirds and then that works.
38:49
So you see that Gauss Jordan elimination actually gives us a way to reveal what the null spaces of a matrix. So it does everything for us not only solves a linear system.
38:59
It finds an inverse when it exists and when it doesn't, it finds the set of all vectors that eight times x equals zero, like either Nell six
39:10
So the key fact that I'm really using here just so that you have this in your hand is that you know if he is a square matrix, like what we had above and the expression above and it's convertible.
39:21
Then I claim that the null space of A prime is the same thing as the null space as be times A prime
39:27
And that's the sense in which, you know, when we have a non 099 empty null space non trivial no space and we don't have infertility Gauss Jordan elimination actual reveals to you why you don't have invert ability because it tells you the full set of all things that are in the null space.
39:44
So that's really neat.
39:46
So in fact, this says because, you know, eight times x A prime times x is zero if and only have been times A prime x is zero.
39:56
And this was related to the answer to question five above so Gauss Jordan elimination as the main takeaway gives us many things that also gives us a way to find the null space.
40:06
So now we're really in good shape. And now what I want to do is I want to connect this back to the column space because I said at the very, you know, beginning of
40:17
This segment that the null space and the column space were very interrelated notion. So let's you know see another way to think about this.
40:28
So, you know, why does there being a non zero vector in the null space actually implies that the colon space isn't everything
40:39
So let's see. Another way to look at this. In fact, that's pretty simple. It's just a little bit you know computationally intensive, but I've done out the computation for you. Let's think about
40:50
This right
40:51
Sorry, what
user avatar
David C Morgan
40:53
Could just go back a little bit on this.
40:56
Yeah, please.
40:59
I need TCP. Yes. Thank you. Yeah.
user avatar
Ankur Moitra
41:02
So here, the fact is that, you know, be a square and and vertical doesn't change the null space. That's the key property.
41:09
It doesn't change the convertibility it doesn't change the null space. So it preserves a lot of invariance. And that's why gas Jordan elimination is making progress.
41:18
Is because it preserves things as we go along. While simplifying it. So this is another wrinkle of that same philosophy that we had way back when, and lecture three
41:29
So now let's think about this. Geometrically, remember we had this notion of the span, which has all of the things we can get from linear combinations of various columns.
41:37
So what does it mean when there's a non zero vector in the column spent in the in the null space.
41:42
Of a matrix A, and it's square. Well, let's think about building up the column space, one by one and see what goes wrong, where Mrs getting all of our three
41:53
Well, when we take the span of just the first column of A, we get a single line we get all of the violations of that one vector
42:02
When we throw in another vector. The second column with a matrix. Say we might get something new because that new vector is off of the line. We started with before. And so now we fill in an entire plane.
42:15
But what happens is that when we add in the third column of the matrix say we don't get anything new.
42:22
Because if you try a general linear combination of the columns. The three columns of the matrix. Say you try all alpha one times the first one. All alpha two times the second one.
42:31
Was all alpha three times the third one, the issue is that you can fold in what happens with the third column using the vector that's in the null space.
42:43
So remember that the vector in the null space while that gives us some vector x so that A times x equals zero.
42:51
So we can rearrange things and let this means as it's expressing the third column of A matrix as a linear combination of the other columns. That's literally what it means for a times x to be equal to zero.
43:05
And what this means. Now is that I can take this last term in the linear combination with this alpha three
43:12
And actually folded into changing the coefficients on the first two vectors step.
43:19
Because I can take this expression, and I can plug it in. So when I rewrite the expression above what I'm actually going to get is, I'm going to get alpha one plus six fifth times alpha three
43:31
Times the first vector. The first column vector in a plus something else where the second coefficient is change. It's no longer alpha two, but it's alpha two plus four fifths times alpha three
43:45
Times this second column in a. So this comes just from taking this expression above and folding it into this expression of what a general linear combination looks like this expression star.
43:59
So this tells you that you might be thinking that you're adding a new vector to the mix. And you're going to get all of our three, but the vector that you added actually lives in the plane that you've already gotten so you don't get anything new.
44:12
So that's the intuition and we see that from a vector in the null space. It tells us how exactly that happens and how we're not getting anything new. And you can express the new thing as something that was old.
44:27
All right, so, you know, putting this all together. What I claim this that you know if a is square
44:37
And n by n, then
44:42
See of a the column space of A is all of our and if and only if the no space of A is the trivial subspace. It only has the vector zero in it.
44:53
This is just a reformulation
44:56
Alright, so now in the five minutes remaining. Actually, I have enough time to do this fun actually anti application. So let's talk about
45:05
Something and data science. Which actually shows up. I mean, I'm going to show it to you in a relatively straightforward way but it comes up all the time, and even more sophisticated things
45:14
It often happens that you know you have a bunch of data and you want to fit it
45:19
Now fitting data actually, you know, you can think about it as a linear system and a lot of context and other cases more complicated optimization problem that we'll get into later.
45:29
But I want to talk about something that's an anti application in the sense that it's something that's really bad that you don't want to do
45:35
In fact, what's worse is not only do you not want to do it, but you want to be aware of these things and use concepts from linear algebra to recognize when other people have done it and use data to lie to you.
45:47
So that's really called overfit so imagine I have a bunch of data points. And let's imagine they're in 2D. So I have, you know, five different data points there each x, y, Paris, I've x one why one all the way up to X five Wi Fi.
46:01
And one thing I couldn't want to do is maybe I want to fit these data points with a polynomial have some degree, maybe a somewhat low degree polynomial. So I want to create some sort of polynomial that when I look at
46:17
You know when I compute P of X one, I get the value why one I compute P of x two, I get the value. Why two and so on.
46:25
So that it passes through all of these data points. So let me first write this as a linear system. But then we're going to reason about what it's Colonel teaching on. Sorry, it's no space teaches us about what overfitting looks like Colonel is another name personal space.
46:41
So can we find a polynomial p of x that you know goes out to degree d. So, we can write it as C zero plus one times x, all the way up to see the times x to the D.
46:53
And we don't know these coefficients see zero see one up to CD. So we have deep plus one on known coefficients. And I claim we can write the problem.
47:02
As fitting the data we're given as a linear system because I can take the coefficients. I want to find c zero up to c d and I can turn them into a vector of dimension d plus one.
47:14
That's this vector. See, and now I can write out rows of A that really just look like what the different mono meals in my polynomial look like at the different data points.
47:25
So the first mono meal is the constant term. So I take this vector. See, and multiply it and I got the contribution C zero
47:32
But then I get see one times x one and then I get see two times x one squared and so on. That's what I get when I take the inner product of the first row of this matrix with this column.
47:43
And that gives me the evaluation of the polynomial p of x at the point x one and that is supposed to be equal to y one. So that's where I get this linear system.
47:55
So just to be clear, this matrix A is a five by d plus one matrix. And I have this vector. See, which is a dimension d plus one and that's supposed to be equal to this five dimensional vector why
48:10
So let's say I get lucky and I, you know, actually can find some coefficients. See that solve this linear system. I can find a polynomial that fits my data.
48:23
And you know one question I have for you is, what does it mean these linear algebra concepts in the real world.
48:29
So what does it mean if I take this matrix and I look at it and I computed smell space. And I figure out that it's no spaces, not just the zero vector.
48:38
So what if it has a non trivial subspace is it's null space. So if there's a non zero vector. And it's null space. Why should I be really worried about the story you're trying to tell me with that particular polynomial
48:52
So the issue is that if you take any
48:55
non zero vector in the null space eat well, not only does the polynomial that's generated by the vector see fit the data, so too does the polynomial generated by c plus he
49:09
Because a time C plus ease the same thing as as a C plus eight and eight times he is equal to zero because that's the definition of the null space.
49:20
So what ends up happening is that he gives me a way to perturb my polynomial. So that it's still passes through the same set of points.
49:28
But by choosing the right perturbation. He I can then get it to fit any other data point that I want.
49:35
So this is really dangerous, right, because you know if I'm trying to fit a low degree polynomial to a bunch of data.
49:41
I can fit all of the data. And that tells a compelling story, but when the null space is non trivial.
49:47
That means I can tell any other story off of it. But I want to I can say, look, I'm giving you a very accurate prediction.
49:53
As fits all of my old data and it predicts that this value, this should be the value way out here.
49:59
So this may sound kind of ridiculous. But the key point that I'm trying to take, you know, have you as a takeaway is that I claim that linear algebra plays a key role in understanding when you're overfit
50:09
It's actually even more subtle linear algebra concepts will come into play that we'll get to notions of stability and condition number
50:16
And this may sound kind of divorced from things that people do and real machine learning.
50:21
But just as a cautionary tale, you know, why in the world would you try and fit a polynomial with more degrees of freedom.
50:27
To a fewer number of data points. Why should I fit a degree 10 polynomial to five different data points.
50:33
Well you know if you've heard of deep learning that I think you know the answer.
50:37
Almost all the time and deep learning when you're using it, you have a million examples, but the way that you fit things as you have 10 million parameters or 100 million parameters. That's the game.
50:50
So the entire game of these things is actually overfitting. So understanding. You know what kinds of concepts and linear algebra and optimization or related
50:58
To how you're dangerously and not dangerously overfitting is really stuff that we're working up to words through these fundamentals. So I'll stop there. And next time we'll be Pablo. So I'll see you in a little bit, but I'll still be here for the lectures and meeting and stuff.